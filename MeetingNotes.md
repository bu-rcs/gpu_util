
# Meeting notes
## *2025-06-02*
Present: Josh

Worked on `qgpu_report`:
1. Looked at XDMOD plots, using "https://xdmod.access-ci.org/" as an example
2. Re-use Ryan's existing plot approach and just modify it (more simple than an interactive webpage like XDMOD)
* per gpu/job graphs
* mem usage
* GPU utilization
* time-series graphs?
3. SCC is down for maintenance, but got `reportgenerator.py` running locally to dig more into data table/plot generation.
4. Report generated when requested, or automatically? Is running time important? Running full `reportgenerator.py` is not "instant", so if it needs to be faster a more direct approach may be needed.
5. Could re-use qacct_gpu approach and pull per-job data vs getting all data and filtering.
6. Might be worth profiling `reportgenerator.py` to see if speed-up possible?

## *2025-05-19*
Present: Josh

Worked on two main things:

1: Reading through Ryan's code to understand best way to output a CSV file like the one generated by original MATLAB code and/or how best to work with data in memory. Useful for queries that aren't already generated in PDF output.

2: Implemented a first version of the `qacct_gpu` command. This command takes the output of `qacct -j <job ID>` and reads the relevant gpustats files to generate summary GPU stats similar to the CPU related info displayed by regular `qacct`. In addition to the selected `qacct` info, it also displays:
1. `num_gpus` Number of GPUs requested for job
2. `gpu_avg_util` Average GPU utilization over the life of the job
3. `gpu_idletime` The total cumulative time the GPUs spent idle at <1% utilization
4. `peak_vram` The max peak VRAM consumed by a GPU in MB
5. `gpu_type` Type of GPU used by job. This could be found based on hostname, but is a bit tedious to get manually.

Originally this command was going to parse the output from Ryan's code, but that would be overkill given we are just interested in a single job and we would like the command to be nearly instant.

## *2025-04-07*
Present: Josh, Katia & Ryan

Ryan has enhanced his report and added reports for the "shared" resources only. Below is Ryan's todo list:  
1. Report by queue;
2. Page 8: Add values for each bar, i.e. 56 (32%)
3. Page 9: Add only percentages for the top portion of each bar( you can place them to the middle right). Use only one digit after a period.
4. Page 10: see if you can add a table to the upper-right corner to specify number of jobs for each count of GPUs.
5. The last 2 pages and page 8: To split on buy-in  vs shared resources, use the column that is "shared" for "*-pub* queues.
6. For all graphs starting from page 5 use "class-user" from Katia's file to diff. between shared vs. buy-in
7. Fix README file to list the right python script name.

Katia's to-do list:  
1. Katia (to think): For overall utilization (for 2 pages of report) where to get buy-in vs. shared information for historical data
2. Katia: ask Mike to add Ryan to the install-group, so the script could be installed as a command-line tool
3. Develop a list of visualizations suitable for each case: GPU over all utilization, project GPU utilization, user GPU utilization, queue GPU utilization
4. What metrics we need to report for memory utilization?

Josh's to-do list:  
1. Check if Ryan's script generates a similar output (for merging datasets) to the one Josh creates in Matlab
2. Check if the script needs any optimization
3. qgpuacct (think if there is any better name for this command): what components should it include?
4. qgpureport (maybe there is a better name for it); Check ACCESS and see how they report various things in XDMOD. This report would potentially create a pdf report with GPU memory and GPU utilization graphs and maybe something else.

## *2025-03-17*   
Present: Katia & Ryan  
1. Ryan discovered a number of jobs (4102432, 4093245, 4102429, 4102431) that ran around January 29, 2024 (timestamp 1706558401) and are present in gpustat files but are not present in the accounting files. Katia's to do: check these jobs and contact Mike if necessary. Another possibly problematic job: 4086404. Ryan will provide more information on it.
2. Ryan will add the GPU name (from the file path) to the dataframe that he creates from these files. This will be useful for debugging purposes. Ryan will also use a username to the merging 2 dataset process (as the job ID is recycled a couple times a year).
3. Ryan will ignore the jobs that are not present in the accounting files (see #1 above) but will report those (as a simple warning message) when merging 2 datasets.
4. Ryan will create a PDF file with graphs (and possibly some explanatory text) for RCS managers to explore GPU node utilization.
5. Starting mid-march, Mike added additional columns to his gpustat files. These columns are: memory usage (total, used in MB?), temperature (gpu current temp), power draw. The last 2 columns can be ignored for right now, but include them in the merged dataset. Maybe later we will figure out what these columnd can be used for. Incorporate these additional fields into the merged dataset and starting april (or probably may) we can include their analysis in the "manager" report;
6. Submit what you have done so far to GitHub (create a pull request)
7. The additional fields should eventually be added to the user's job report command-line tool. We want to capture:
   - mean, median, max GPU utilization
   - mean, median, max GPU memory usage
   - max GPU idle time
   - total GPU idle time
   - start and end GPU idle time

## *2025-03-10*  
Present: Katia & Ryan  
1. Ryan completed the standard read python function and created pull request for RCShelpers_py repo (into main branch)
2. Pull request for gpu_util (into ryan branch)
3. Ryan's Todo:
      - Try to find GPUs jobs that are longer than a month (there are some in 2024) and see if they are not lost in the "year" analysis
      - One more time interval to handle: from date to date (vs particular year or a month)
      - Other items from Katia's list [https://github.com/bu-rcs/gpu_util/blob/katia/Questions.md]
4. Katia's todo: Check with Mike on "short" lines in gpustats file

## *2025-02-24*  
Present: Katia & Ryan
 

Ryan has made a huge progress: he wrote standard functions to read csv and feather account files, he wrote a function that reads only gpu records from the csv accounting file and wrote the functions that merges accounting file with the gpu data files create by Mike

Ryan's next steps are:


1. Generic Python functions that reads all three types of accounting file and returns a list
   - leave read_csv function returning a "generator". ; Write an example of using this function and a) "time it" and also separately convert to a list and print the number of records (the length of the list)
   - create a similar function for the regular accounting file (no extension)
   - read_feather:
   - investigate why csv file is smaller.

   - Do pull request into https://github.com/bu-rcs/rcshelpers_py repo
   - email/slack katia when you time all 3 as batch jobs.
2. For the read gpu recrods only
   - read_gpu_records _with _grep should go to the repo https://github.com/bu-rcs/gpu_util
   - Just for a test: read_gpu records from feather file and see if the number of records is the same if you use your read_gpu_recrods_with grep() function;
3. For the handling GPU taks:
   - JOsh's file /projectnb/rcsmetrics/gpu_util/data/010124_112524.txt

   - In addition to Josh's columns, could you make JobID and JobTask as separate integer columns
   - Create several functions: create "merged" filed
       a) merge everything for a particular year
       b) merge everything for a particular month
       c) merge everything for a particular month for a specific list of projects

       d) start creating text and visual reports : wallclocktime (in hours), wallclock in GPU-hours; also try to split them by "interactive" vs "batch jobs": job_name.str.startswith("ood") | (job_name == "QRLOGIN")
     e) Shared GPU utilization over the year; Buy-in utilization over the year; All utilization over the year;Katia's file: /projectnb/scv/utilization/katia/queue_info.csv


## *2025-02-10*
1. Josh ToDo: Add "Task ID" column to the "input" (for Katia) file
2. Katia ToDo: Process the "input" file correctly to make sure records are not assumed to be jobs, as each recrods correspond to a GPU (so for 4 GPU-job there are 4 records)
3. Katia: Ask Ryan Gilbert to create a Python function that reads in accounting file as fast as possible.
4. Josh: Find a "problematic" GPUstats file with missing data/column

## *2025-01-13*

- Katia: fix the calculation of the number and % of jobs that do not use GPUs. Need to use comp_tot column
- Josh: Finalize the names of the columns in the file
- Katia: for the wallclock (batch vs. interactive) GPU job graphs, add the number of GPUs:
      - one graph can be stacked (by the number of GPUs used )
      - the other graph can have y axis gpu-hours (based on the number of GPUs used for the jobs
- Note for Katia: number of hosts is not the same as the number of GPUs!

## *12/9/24*
- Discuss with Charlie what other statistics we need for the Mike's files:
    - memory usage (and probably total memory so it is easier)
    - temperature ?
    - power?
 
 - Created Questions.md file to start collecting questions we want to answer.
 - We should now think about what kind of files we want to generate, where to store them and disscuss the naming conventions for them:
   - For the files that are about RCS resource utilization we will generate monthly files (with the naming convention similar to Mike) and this can be used by the Shiny App
   - For the User/queue/project utilization we will (at least for now) will generate a static report - given the start and end date and the information about username, project name or queue name.

## *11/25/24*
- Naming convention for the file names (including intermediate csv); Do we give period of time (start day-end day) or month name.
- Josh and Katia will create branches and will be working within them

## *11/18/24*  
Schedule
- Monday 10am-12:30pm every other week
- Josh schedule meeting
- Starting on 11/25 <br>
"Beta" MATLAB version of GPU util output here: /projectnb/rcsmetrics/gpu_util/data  
Repo  
- For not just code, but project docs and meeting notes
Intern
- Join January or February

